# ðŸ“‹ TECHNICAL DIRECTIVE: Phase 3A Hybrid Load

> **Tá»«:** AI Technical Lead  
> **Äáº¿n:** Backend Development Team  
> **NgÃ y:** 2025-12-26  
> **Ref:** HINTO-CANDLE-003A  
> **Priority:** HIGH

---

## ðŸŽ¯ Má»¤C TIÃŠU

Implement SOTA Hybrid Data Layer: SQLite first, Binance fallback.

**Váº¥n Ä‘á» hiá»‡n táº¡i:**
- SQLite SAVES candles nhÆ°ng `_load_historical_data()` KHÃ”NG READ tá»« SQLite
- Má»—i láº§n restart luÃ´n gá»i Binance API (cháº­m, dependency external)

---

## ðŸ“ Báº®T Äáº¦U Tá»ª ÄÃ‚U

```
1. Má»Ÿ file: src/application/services/realtime_service.py
2. ThÃªm helper _load_candles_hybrid() method
3. Update _load_historical_data() Ä‘á»ƒ dÃ¹ng hybrid method
4. Test
```

---

## ðŸ”§ TASKS

### Task 1: ThÃªm `_load_candles_hybrid` helper method

**File:** `src/application/services/realtime_service.py`  
**Vá»‹ trÃ­:** ThÃªm TRÆ¯á»šC method `_load_historical_data()` (khoáº£ng line 195)

```python
    def _load_candles_hybrid(self, timeframe: str, limit: int = 100) -> List[Candle]:
        """
        SOTA Hybrid Data Layer: Load candles from SQLite first, Binance fallback.
        
        Pattern: Read-through cache
        - L1: In-memory (populated by this method)
        - L2: SQLite (check first)
        - L3: Binance API (fallback + write-through)
        
        Args:
            timeframe: '1m', '15m', or '1h'
            limit: Number of candles to load
            
        Returns:
            List of Candle objects, sorted by timestamp ascending
        """
        local_candles = []
        
        # Step 1: Try SQLite first (L2 cache)
        if self._market_data_repository:
            try:
                market_data_list = self._market_data_repository.get_latest_candles(
                    timeframe, limit
                )
                local_candles = [md.candle for md in market_data_list]
                # Sort ascending (oldest first) - SQLite returns DESC
                local_candles = sorted(local_candles, key=lambda c: c.timestamp)
                
                if local_candles:
                    self.logger.info(f"ðŸ“¦ SQLite HIT: {len(local_candles)}/{limit} {timeframe} candles")
            except Exception as e:
                self.logger.warning(f"âš ï¸ SQLite read failed for {timeframe}: {e}")
        
        # Step 2: Check if we have enough data (80% threshold)
        threshold = int(limit * 0.8)
        if len(local_candles) >= threshold:
            return local_candles
        
        # Step 3: SQLite miss - fetch from Binance (L3)
        self.logger.info(f"ðŸ“¡ SQLite MISS for {timeframe} ({len(local_candles)}/{limit}), fetching from Binance...")
        
        try:
            external_candles = self.rest_client.get_klines(
                symbol=self.symbol,
                interval=timeframe,
                limit=limit
            )
            
            if not external_candles:
                self.logger.warning(f"No external data for {timeframe}")
                return local_candles  # Return whatever we have
            
            # Step 4: Merge local + external, deduplicate by timestamp
            merged = self._merge_candles(local_candles, external_candles)
            
            # Step 5: Write-through - save new candles to SQLite
            if self._market_data_repository:
                local_timestamps = {c.timestamp for c in local_candles}
                new_candles = [c for c in merged if c.timestamp not in local_timestamps]
                
                if new_candles:
                    self.logger.info(f"ðŸ’¾ Write-through: Saving {len(new_candles)} new {timeframe} candles to SQLite")
                    for candle in new_candles:
                        try:
                            self._market_data_repository.save_candle_simple(candle, timeframe)
                        except Exception as e:
                            self.logger.error(f"Failed to save candle: {e}")
            
            return merged
            
        except Exception as e:
            self.logger.error(f"Binance fetch failed for {timeframe}: {e}")
            return local_candles  # Return whatever we have from SQLite
    
    def _merge_candles(self, local: List[Candle], external: List[Candle]) -> List[Candle]:
        """
        Merge local and external candles, deduplicate by timestamp.
        
        Priority: External (source of truth) for conflicts
        """
        # Create map by timestamp, external overwrites local
        candle_map = {}
        
        for candle in local:
            candle_map[candle.timestamp] = candle
        
        for candle in external:
            candle_map[candle.timestamp] = candle  # Overwrites if exists
        
        # Sort by timestamp ascending
        merged = sorted(candle_map.values(), key=lambda c: c.timestamp)
        return merged
```

---

### Task 2: Update `_load_historical_data()` method

**File:** `src/application/services/realtime_service.py`  
**Vá»‹ trÃ­:** Replace toÃ n bá»™ method `_load_historical_data()` (lines 199-269)

```python
    async def _load_historical_data(self) -> None:
        """
        SOTA Hybrid Load: SQLite first, Binance fallback.
        
        This populates the buffer with recent candles so the dashboard
        shows data immediately instead of waiting for the first candle to close.
        
        Architecture:
        - L1: In-memory deques (fastest, volatile)
        - L2: SQLite (fast, persistent)
        - L3: Binance REST API (slow, source of truth)
        """
        try:
            self.logger.info("ðŸš€ Loading historical candles (SOTA Hybrid)...")
            
            # 1. Load 1m candles
            candles_1m = self._load_candles_hybrid('1m', 100)
            if candles_1m:
                for candle in candles_1m:
                    self._candles_1m.append(candle)
                    self.aggregator.add_candle_1m(candle, is_closed=True)
                self._latest_1m = candles_1m[-1]
                self.logger.info(f"âœ… Loaded {len(candles_1m)} 1m candles")
            else:
                self.logger.warning("No 1m data available")
            
            # 2. Load 15m candles (exclude last = incomplete)
            candles_15m = self._load_candles_hybrid('15m', 100)
            if candles_15m and len(candles_15m) > 1:
                completed_15m = candles_15m[:-1]  # Exclude incomplete
                for candle in completed_15m:
                    self._candles_15m.append(candle)
                self._latest_15m = completed_15m[-1]
                self.logger.info(f"âœ… Loaded {len(completed_15m)} 15m candles")
            
            # 3. Load 1h candles (exclude last = incomplete)
            candles_1h = self._load_candles_hybrid('1h', 100)
            if candles_1h and len(candles_1h) > 1:
                completed_1h = candles_1h[:-1]
                for candle in completed_1h:
                    self._candles_1h.append(candle)
                self._latest_1h = completed_1h[-1]
                self.logger.info(f"âœ… Loaded {len(completed_1h)} 1h candles")
            
            self.logger.info("âœ… Historical data loaded successfully (SOTA Hybrid)")
            
        except Exception as e:
            self.logger.error(f"Error loading historical data: {e}")
            # Don't fail - continue with WebSocket streaming
```

---

## âœ… VERIFICATION

```bash
# 1. Delete old SQLite database (fresh start)
del crypto_data.db

# 2. Start backend
python -m src.main

# 3. Check logs - should see SQLite MISS (fresh DB)
ðŸ“¡ SQLite MISS for 1m (0/100), fetching from Binance...
ðŸ’¾ Write-through: Saving 100 new 1m candles to SQLite

# 4. Wait 15-30 minutes for candles to accumulate, then Restart backend

# 5. Check logs - should see SQLite HIT
ðŸ“¦ SQLite HIT: 95/100 1m candles
ðŸ“¦ SQLite HIT: 8/100 15m candles
```

---

## ðŸ“Š EXPECTED BEHAVIOR

| Scenario | Before | After |
|----------|--------|-------|
| First startup | Binance only | Binance + save to SQLite |
| Restart with data | Binance only | SQLite first (fast) |
| Binance API down | No data | Use SQLite cache |

---

*Priority: HIGH - Core persistence fix*
